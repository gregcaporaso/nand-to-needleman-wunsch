#!/usr/bin/env python

import sys
import pathlib
import glob
from enum import Enum
import tempfile
import collections


def _print_usage(program_name):
    usage = (
        f"\n{program_name} usage instructions:\n\n"
        f"  {program_name} (--help | -h)\n"
        f"     print help text and exit\n\n"
        f"  {program_name} (--verbose) in.asm\n"
        f"     assemble in.asm to in.hack\n\n"
        f"  {program_name} (--verbose) my/path/in.asm\n"
        f"     assemble my/path/in.asm to my/path/in.hack\n\n"
        "✌️")
    print(usage)


T = Enum('TokenType',
         ['KEYWORD',
          'SYMBOL',
          'IDENTIFIER',
          'INT_CONST',
          'STRING_CONST'])


TOKEN_TAGS = {T.KEYWORD.name: 'keyword',
              T.SYMBOL.name: 'symbol',
              T.IDENTIFIER.name: 'identifier',
              T.INT_CONST.name: 'integerConstant',
              T.STRING_CONST.name: 'stringConstant'}

TAG_TYPES = {'keyword': T.KEYWORD,
             'symbol': T.SYMBOL,
             'identifier': T.IDENTIFIER,
             'integerConstant': T.INT_CONST,
             'stringConstant': T.STRING_CONST}

K = Enum('Keyword',
         ['CLASS',
          'METHOD',
          'FUNCTION',
          'CONSTRUCTOR',
          'INT',
          'BOOLEAN',
          'CHAR',
          'VOID',
          'VAR',
          'STATIC',
          'FIELD',
          'LET',
          'DO',
          'IF',
          'ELSE',
          'WHILE',
          'RETURN',
          'TRUE',
          'FALSE',
          'NULL',
          'THIS'])

I = Enum('IdentifierKind', ['STATIC', 'FIELD', 'ARG', 'VAR'])


class JackSyntaxError(ValueError):
    pass


class JackTokenizer(object):

    _out_ex = 'T.xml'
    _symbols = set(list('{}()[].,;+-*/&|<>=~'))
    _keywords = set(['class',
                     'method',
                     'function',
                     'constructor',
                     'int',
                     'boolean',
                     'char',
                     'void',
                     'var',
                     'static',
                     'field',
                     'let',
                     'do',
                     'if',
                     'else',
                     'while',
                     'return',
                     'true',
                     'false',
                     'null',
                     'this'])

    def __init__(self, verbose=False):
        self._verbose = verbose

    def __call__(self, infp):
        outfp = infp.parent / \
                    pathlib.Path(f"{infp.with_suffix('').name}{self._out_ex}")

        if self._verbose:
            print(f"Tokenizing {infp} → {outfp}")

        with tempfile.TemporaryFile(mode='w+') as inf, \
                open(infp) as commenty_inf, \
                open(outfp, 'w') as outf:
            self._strip_comments(commenty_inf, inf)
            outf.write('<tokens>\n')
            while True:
                c = inf.read(1)
                if c == '':
                    break
                if c.isspace():
                    continue
                token_type, token = self._identify_token(c, inf)
                outf.write(
                    f"{self._format_token_xml(token_type, token)}\n")
            outf.write('</tokens>\n')

        return outfp

    def _raise_error(self, error_message):
        raise ValueError(error_message)

    def _identify_token(self, c, inf):
        if c in self._symbols:
            return T.SYMBOL, self._process_symbol(c)
        elif c == '"':
            return T.STRING_CONST, self._process_string_constant(inf)
        elif c.isdigit():
            return T.INT_CONST, self._process_int_constant(c, inf)
        else:
            return self._process_keyword_or_identifier(c, inf)

    def _format_token_xml(self, token_type, token):
        tag = TOKEN_TAGS[token_type.name]
        return f"<{tag}> {token} </{tag}>"

    def _process_symbol(self, c):
        if c == '<':
            return '&lt;'
        elif c == '>':
            return '&gt;'
        elif c == '&':
            return '&amp;'
        else:
            return c

    def _process_string_constant(self, inf):
        result = []
        while True:
            c = inf.read(1)
            if c == '"':
                break
            else:
                result.append(c)
        return ''.join(result)

    def _process_int_constant(self, c, inf):
        result = [c]
        while True:
            c = inf.read(1)
            if not c.isdigit():
                inf.seek(inf.tell() - 1)
                break
            else:
                result.append(c)
        result = ''.join(result)

        if int(result) > 32767:
            self._raise_error("Integer values cannot be larger than 32,767, "
                              f"but encountered {result}.")

        return result

    def _process_keyword_or_identifier(self, c, inf):
        result = [c]
        while True:
            c = inf.read(1)
            if not (c.isalnum() or c == '_'):
                inf.seek(inf.tell() - 1)
                break
            else:
                result.append(c)
        result = ''.join(result)

        if result in self._keywords:
            return T.KEYWORD, result
        else:
            return T.IDENTIFIER, result

    def _strip_comments(self, inf, outf):
        inside_comment = False
        for line in inf:
            if not inside_comment:
                start_comment_index = line.find('//')
                if start_comment_index >= 0:
                    line = line[:start_comment_index]

            line = line.strip()

            if line == '':
                continue

            if not inside_comment:
                start_comment_index = line.find('/*')
                if start_comment_index == -1:
                    outf.write(line)
                    outf.write('\n')
                else:
                    inside_comment = True
                    pre_comment = line[:start_comment_index]
                    outf.write(pre_comment)
                    outf.write('\n')
                    line = line[start_comment_index + 2:]

            if inside_comment:
                end_comment_index = line.find('*/')
                if end_comment_index >= 0:
                    inside_comment = False
                    post_comment = line[end_comment_index + 2:]
                    outf.write(post_comment)
                    outf.write('\n')
        outf.seek(0)


SymbolTableEntry = collections.namedtuple('SymbolTableEntry',
                                          ['name', 'type', 'kind', 'index'])


class SymbolTable(object):

    def __init__(self):
        self._table = {}
        self._counts = {i:0 for i in I}

    def define(self, name, type, kind):
        if name in self._table:
            raise JackSyntaxError(
                f"Identifier `{name}` already defined as {self[name].kind} "
                "variable.")

        if kind not in I:
            raise JackSyntaxError(f"Unknown kind of variable {kind}.")

        self._table[name] = SymbolTableEntry(name=name,
                                             type=type,
                                             kind=kind,
                                             index=self._counts[kind])
        self._counts[kind] += 1

    def __getitem__(self, name):
        if name not in self._table:
            raise JackSyntaxError(
                f"Identifier {name} is referenced but not defined.")
        return self._table[name]

    def display(self):
        for e in self._table.values():
            print(f"{e.name:<10} | {str(e.kind):>25} | "
                  f"{e.type:>10} | {e.index:>3}")


class CompilationEngine(object):

    _out_ex = '.xml'
    _next_token_type = None
    _next_token = None
    _builtin_types = set(['int', 'char', 'boolean'])
    _keyword_constants = set(['true', 'false', 'null', 'this'])
    _unary_ops = set(['~', '-'])
    _ops = set(['+', '-', '*', '/', '&amp;', '|', '&lt;', '&gt;', '='])

    def __init__(self, inf, outf, verbose=False):
        self._verbose = verbose
        self._inf = inf
        self._outf = outf

    def __call__(self):
        self._advance_token()
        self._compile_class()

    def _advance_token(self):
        # ensure that this generally works for different types of
        # self._inf, e.g., so streams rather than files can be passed as
        # input
        self._next_token_type, self._next_token = \
            self._parse_xmlT_line(self._inf.readline())

    def _parse_xmlT_line(self, line):
        line = line.strip()
        if line == '<tokens>':
            return self._parse_xmlT_line(self._inf.readline())
        elif line == '</tokens>':
            return None, None
        else:
            try:
                open_tag, temp = line.split(maxsplit=1)
                token, _ = temp.rsplit(maxsplit=1)
            except ValueError:
                raise ValueError(f"Failed to parse xmlT line:\n {line}")

            try:
                token_type = TAG_TYPES[open_tag[1:-1]]
            except KeyError:
                raise ValueError(f"Unknown tag encountered: {open_tag[1:-1]}")

            return token_type, token

    def _process_token(self, *args, start='', end='\n', error_message=None):
        if self._next_token in args or len(args) == 0:
            tag = TOKEN_TAGS[self._next_token_type.name]
            self._outf.write(
                f"{start}<{tag}> {self._next_token} </{tag}>{end}")
            self._advance_token()
        else:
            if error_message is None:
                error_message = (f"Next token {self._next_token} is not in "
                                 "set of allowed next tokens:\n"
                                 f" ({', '.join(args)})")
            raise JackSyntaxError(error_message)

    def _process_identifier(self, start='', end='\n', error_message=None):
        if self._next_token_type != T.IDENTIFIER:
            if error_message is None:
                error_message = \
                    ("Expected identifier but found "
                     f"{str(self._next_token_type)}: {self._next_token}")
            raise JackSyntaxError(error_message)

        tag = TOKEN_TAGS[self._next_token_type.name]

        self._outf.write(f"<{tag}> {self._next_token} </{tag}>{end}")
        self._advance_token()

    def _process_type(self, start='', end='\n'):
        tag = TOKEN_TAGS[self._next_token_type.name]
        if self._next_token_type == T.KEYWORD:
            if self._next_token in self._builtin_types:
                self._outf.write(
                    f"{start}<{tag}> {self._next_token} </{tag}>{end}")
            else:
                raise JackSyntaxError(
                    "Next token {self._next_token} is not in set of built-in "
                    f"types:\n ({', '.join(self._builtin_types)})")
        elif self._next_token_type == T.IDENTIFIER:
            self._outf.write(
                f"{start}<{tag}> {self._next_token} </{tag}>{end}")
        else:
            raise JackSyntaxError(
                f"Next token {self._next_token} is not a built-in type or "
                "class name.")
        self._advance_token()

    def _compile_class(self):
        self._outf.write('<class>\n')
        self._process_token(
            'class', end='\n',
            error_message='.jack files must begin with class definition.')
        self._process_identifier()
        self._process_token('{', end='\n')
        self._compile_class_var_dec()
        self._compile_subroutine_dec()
        self._process_token('}', end='\n')
        self._outf.write('</class>\n')

    def _compile_class_var_dec(self):
        class_var_categories = ('static', 'field')
        while self._next_token in class_var_categories:
            self._outf.write(' <classVarDec>\n')
            self._process_token(*class_var_categories)
            self._process_type()
            self._process_identifier()
            while self._next_token != ';':
                self._process_token(',')
                self._process_identifier()
            self._process_token(';')
            self._outf.write('</classVarDec>\n')

    def _compile_subroutine_dec(self):
        subroutine_categories = ('constructor', 'function', 'method')
        while self._next_token in subroutine_categories:
            self._outf.write(' <subroutineDec>\n')
            self._process_token(*subroutine_categories)
            if self._next_token == 'void':
                self._process_token('void')
            else:
                self._process_type()
            self._process_identifier()
            self._process_token('(')
            self._compile_parameter_list()
            self._process_token(')')
            self._compile_subroutine_body()

            self._outf.write(' </subroutineDec>\n')

    def _compile_parameter_list(self):
        self._outf.write(' <parameterList>\n')
        if self._next_token != ')':
            self._process_type()
            self._process_identifier()
            while self._next_token != ')':
                self._process_token(',')
                self._process_type()
                self._process_identifier()
        self._outf.write(' </parameterList>\n')

    def _compile_subroutine_body(self):
        self._outf.write(' <subroutineBody>\n')
        self._process_token('{')
        while self._next_token == 'var':
            self._compile_var_dec()
        self._compile_statements()
        self._process_token('}')
        self._outf.write(' </subroutineBody>\n')

    def _compile_var_dec(self):
        self._outf.write(' <varDec>\n')
        self._process_token('var')
        self._process_type()
        self._process_identifier()
        while self._next_token != ';':
            self._process_token(',')
            self._process_identifier()
        self._process_token(';')
        self._outf.write(' </varDec>\n')

    def _compile_statements(self):
        self._outf.write(' <statements>\n')
        while self._next_token != '}':
            if self._next_token == 'let':
                self._outf.write(' <letStatement>\n')
                self._process_token('let')
                self._process_identifier()
                if self._next_token == '[':
                    self._process_token('[')
                    self._compile_expression()
                    self._process_token(']')
                self._process_token('=')
                self._compile_expression()
                self._process_token(';')
                self._outf.write(' </letStatement>\n')
            elif self._next_token == 'if':
                self._outf.write(' <ifStatement>\n')
                self._process_token('if')
                self._process_token('(')
                self._compile_expression()
                self._process_token(')')
                self._process_token('{')
                self._compile_statements()
                self._process_token('}')
                if self._next_token == 'else':
                    self._process_token('else')
                    self._process_token('{')
                    self._compile_statements()
                    self._process_token('}')
                self._outf.write(' </ifStatement>\n')
            elif self._next_token == 'while':
                self._outf.write(' <whileStatement>\n')
                self._process_token('while')
                self._process_token('(')
                self._compile_expression()
                self._process_token(')')
                self._process_token('{')
                self._compile_statements()
                self._process_token('}')
                self._outf.write(' </whileStatement>\n')
            elif self._next_token == 'do':
                self._outf.write(' <doStatement>\n')
                self._process_token('do')
                # the book recommends using compile_term here, but I feel
                # like this could let syntax errors slip through.
                self._process_subroutine_call()
                self._process_token(';')
                self._outf.write(' </doStatement>\n')
            elif self._next_token == 'return':
                self._outf.write(' <returnStatement>\n')
                self._process_token('return')
                if self._next_token != ';':
                    self._compile_expression()
                self._process_token(';')
                self._outf.write(' </returnStatement>\n')
            else:
                raise JackSyntaxError(f"Unexpected token {self._next_token} "
                                      "while scanning for next statement "
                                      "type.")
        self._outf.write(' </statements>\n')

    def _compile_expression(self):
        self._outf.write(' <expression>\n')
        self._compile_term()
        while self._next_token in self._ops:
            self._process_token()
            self._compile_term()
        self._outf.write(' </expression>\n')

    def _compile_expression_list(self):
        n_expressions = 0
        self._outf.write(' <expressionList>\n')
        if self._next_token != ')':
            n_expressions += 1
            self._compile_expression()
            while self._next_token == ',':
                self._process_token(',')
                self._compile_expression()
        self._outf.write(' </expressionList>\n')
        return n_expressions

    def _process_subroutine_call(self):
        if self._next_token_type != T.IDENTIFIER:
            raise JackSyntaxError(
                "Invalid token type in subroutine call: "
                f"{self._next_token_type}")
        self._process_identifier()
        if self._next_token == '.':
            self._process_token('.')
            self._process_identifier()
        self._process_token('(')
        _ = self._compile_expression_list()
        self._process_token(')')

    def _compile_term(self):
        self._outf.write(' <term>\n')
        if self._next_token_type == T.IDENTIFIER:
            self._process_identifier()
            if self._next_token == '[':
                # array access
                self._process_token('[')
                self._compile_expression()
                self._process_token(']')
            elif self._next_token == '(':
                # this.method() call
                self._process_token('(')
                _ = self._compile_expression_list()
                self._process_token(')')
            elif self._next_token == '.':
                # not_this.method() call
                self._process_token('.')
                self._process_identifier()
                self._process_token('(')
                _ = self._compile_expression_list()
                self._process_token(')')
            else:
                # simple variable access
                pass
        elif self._next_token_type == T.KEYWORD:
            self._process_token(*self._keyword_constants)
        elif self._next_token_type in (T.INT_CONST, T.STRING_CONST):
            self._process_token()
        elif self._next_token == '(':
            self._process_token('(')
            self._compile_expression()
            self._process_token(')')
        elif self._next_token in self._unary_ops:
            self._process_token()
            self._compile_term()
        else:
            raise JackSyntaxError(
                f"Unexpected token while processing term: {self._next_token}")
        self._outf.write(' </term>\n')


class JackAnalyzer(object):

    _in_ex = '.jack'

    def __init__(self, verbose=False):
        self._verbose = verbose
        self._tokenizer = JackTokenizer(verbose)

    def __call__(self, input_path):
        jackfps = self._get_infps(input_path)
        xmlTfps = []
        for jackfp in jackfps:
            xmlTfps.append(self._tokenizer(jackfp))

        xmlfps = []
        for xmlTfp in xmlTfps:
            # remove the T from xT.xml
            xmlfp = xmlTfp.parent / \
                        pathlib.Path(str(xmlTfp.stem)[:-1]).with_suffix('.xml')

            if self._verbose:
                print(f"Compiling {xmlTfp} → {xmlfp}")

            with open(xmlTfp) as inf, open(xmlfp, 'w') as outf:
                compiler = CompilationEngine(inf, outf, verbose)
                compiler()

            xmlfps.append(xmlfp)

    def _raise_error(self, error_message):
        raise ValueError(error_message)

    def _get_infps(self, input_path):
        iex = self._in_ex
        input_path = pathlib.Path(input_path)

        if not input_path.exists():
            raise ValueError(f"Input path doesn't exist: {input_path}")

        if input_path.is_dir():
            input_fps = [pathlib.Path(fp)
                         for fp in glob.glob(str(input_path / f'*{iex}'))]
            if len(input_fps) == 0:
                raise ValueError(
                    f"No input {iex} files found in {input_path}.")
        else:
            if not input_path.suffix == iex:
                raise ValueError(
                    f"Input path must have extension {iex}, "
                    f"but found {input_path.suffix}")
            input_fps = [input_path]

        if self._verbose:
            print("Input file(s):")
            for inf in input_fps:
                print(f' {inf}')
            print()

        return input_fps


if __name__ == "__main__":
    if len(sys.argv) == 1 or '--help' in sys.argv or '-h' in sys.argv:
        _print_usage(program_name=pathlib.Path(sys.argv[0]).name)
        exit(0)

    verbose = '--verbose' in sys.argv
    input_path = pathlib.Path(sys.argv[-1])

    ja = JackAnalyzer(verbose)
    ja(input_path)
